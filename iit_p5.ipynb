{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f1557c-8b6a-41e9-9e88-f4e5384da20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns:\n",
      "['Junk', 'InteriorsStyle', 'PriceIndex8', 'ListDate', 'Material', 'PriceIndex9', 'Agency', 'AreaIncomeType', 'EnvRating', 'PriceIndex7', 'ExpeditedListing', 'PriceIndex4', 'PriceIndex1', 'PriceIndex6', 'PRIMEUNIT', 'Channel', 'Zip', 'InsurancePremiumIndex', 'PlotType', 'Architecture', 'PriceIndex3', 'Region', 'PriceIndex5', 'SubModel', 'Facade', 'State', 'NormalisedPopulation', 'BuildYear', 'RegionType', 'PropertyAge', 'PriceIndex2']\n",
      "\n",
      "Test columns:\n",
      "['InteriorsStyle', 'PriceIndex8', 'ListDate', 'Material', 'PriceIndex9', 'Agency', 'AreaIncomeType', 'EnvRating', 'PriceIndex7', 'ExpeditedListing', 'PriceIndex4', 'PriceIndex1', 'PriceIndex6', 'PRIMEUNIT', 'Channel', 'Zip', 'InsurancePremiumIndex', 'PlotType', 'Architecture', 'PriceIndex3', 'Region', 'PriceIndex5', 'SubModel', 'Facade', 'State', 'NormalisedPopulation', 'BuildYear', 'RegionType', 'PropertyAge', 'PriceIndex2']\n",
      "\n",
      "Train shape: (62035, 31)\n",
      "Test shape: (10948, 30)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- Load data ---\n",
    "train = pd.read_csv(r'E:/IITK/P5 data/Property_train.csv')\n",
    "test = pd.read_csv(r'E:/IITK/P5 data/Property_test_share.csv')\n",
    "\n",
    "# Display column names\n",
    "print(\"Train columns:\")\n",
    "print(train.columns.tolist())\n",
    "\n",
    "print(\"\\nTest columns:\")\n",
    "print(test.columns.tolist())\n",
    "\n",
    "# Optionally check shapes\n",
    "print(f\"\\nTrain shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b25b320-4f0f-43af-830a-2462953be8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\sssso\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\sssso\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.8/1.5 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 10.2 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f56608ec-ef23-46f3-8452-34a39120520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 6082, number of negative: 43546\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 54766\n",
      "[LightGBM] [Info] Number of data points in the train set: 49628, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122552 -> initscore=-1.968484\n",
      "[LightGBM] [Info] Start training from score -1.968484\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's auc: 0.888638\tvalid_1's auc: 0.755756\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's auc: 0.876997\tvalid_1's auc: 0.756431\n",
      "\n",
      "Validation ROC-AUC: 0.7564\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 7602, number of negative: 54433\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 62346\n",
      "[LightGBM] [Info] Number of data points in the train set: 62035, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122544 -> initscore=-1.968559\n",
      "[LightGBM] [Info] Start training from score -1.968559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "# --- Save categorical columns ---\n",
    "cat_cols = train.select_dtypes(['object']).columns.tolist()\n",
    "\n",
    "# --- Mark and combine ---\n",
    "train['data'] = 'train'\n",
    "test['data'] = 'test'\n",
    "test['Junk'] = np.nan\n",
    "\n",
    "all_data = pd.concat([train, test], axis=0, sort=False)\n",
    "\n",
    "# --- Split back ---\n",
    "x_train = all_data[all_data['data'] == 'train'].drop(['Junk', 'data'], axis=1)\n",
    "y_train = all_data[all_data['data'] == 'train']['Junk']\n",
    "x_test = all_data[all_data['data'] == 'test'].drop(['Junk', 'data'], axis=1)\n",
    "\n",
    "# --- Convert object columns to category ---\n",
    "for col in cat_cols:\n",
    "    if col in x_train.columns:\n",
    "        x_train[col] = x_train[col].astype('category')\n",
    "        x_test[col] = x_test[col].astype('category')\n",
    "\n",
    "# --- Validation split ---\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# --- LightGBM dataset ---\n",
    "lgb_train = lgb.Dataset(X_tr, y_tr)\n",
    "lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "# --- LightGBM parameters ---\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'num_leaves': 31,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# --- Train model ---\n",
    "print(\"\\nTraining LightGBM...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Validate ---\n",
    "val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "val_auc = roc_auc_score(y_val, val_pred)\n",
    "print(f\"\\nValidation ROC-AUC: {val_auc:.4f}\")\n",
    "\n",
    "# --- Train final model on full data ---\n",
    "full_train = lgb.Dataset(x_train, y_train)\n",
    "final_model = lgb.train(\n",
    "    params,\n",
    "    full_train,\n",
    "    num_boost_round=model.best_iteration\n",
    ")\n",
    "\n",
    "# --- Predict on test ---\n",
    "test_pred = final_model.predict(x_test, num_iteration=final_model.best_iteration)\n",
    "\n",
    "# --- Create synthetic Id ---\n",
    "test_id = np.arange(len(test))\n",
    "\n",
    "# --- Save submission ---\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_id,\n",
    "    'Junk': test_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6ec051d-b6ab-48aa-a111-bf927c1ba65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(r'E:/IITK/P5 data/submissio.csv', index=False)\n",
    "print(\"Saved:submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "281ef215-c2dd-4cff-9d8d-4424cd155288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (62035, 31)\n",
      "Test shape: (10948, 30)\n",
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 6082, number of negative: 43546\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2780\n",
      "[LightGBM] [Info] Number of data points in the train set: 49628, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122552 -> initscore=-1.968484\n",
      "[LightGBM] [Info] Start training from score -1.968484\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's auc: 0.885809\tvalid_1's auc: 0.760611\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's auc: 0.85191\tvalid_1's auc: 0.763372\n",
      "\n",
      "Validation ROC-AUC: 0.7634\n",
      "[LightGBM] [Info] Number of positive: 7602, number of negative: 54433\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2788\n",
      "[LightGBM] [Info] Number of data points in the train set: 62035, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122544 -> initscore=-1.968559\n",
      "[LightGBM] [Info] Start training from score -1.968559\n",
      "Saved:submission.csv\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- Load data ---\n",
    "train = pd.read_csv(r'E:/IITK/P5 data/Property_train.csv')\n",
    "test = pd.read_csv(r'E:/IITK/P5 data/Property_test_share.csv')\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# --- Save categorical columns ---\n",
    "cat_cols = train.select_dtypes(['object']).columns.tolist()\n",
    "\n",
    "# --- Mark and combine ---\n",
    "train['data'] = 'train'\n",
    "test['data'] = 'test'\n",
    "test['Junk'] = np.nan\n",
    "\n",
    "all_data = pd.concat([train, test], axis=0, sort=False)\n",
    "\n",
    "# --- Group rare categories ---\n",
    "for col in cat_cols:\n",
    "    freq = all_data[col].value_counts()\n",
    "    rare = freq[freq < 50].index\n",
    "    all_data[col] = all_data[col].replace(rare, 'Rare')\n",
    "\n",
    "# --- Convert object columns to category ---\n",
    "for col in cat_cols:\n",
    "    all_data[col] = all_data[col].astype('category')\n",
    "\n",
    "# --- Split back ---\n",
    "x_train = all_data[all_data['data'] == 'train'].drop(['Junk', 'data'], axis=1)\n",
    "y_train = all_data[all_data['data'] == 'train']['Junk']\n",
    "x_test = all_data[all_data['data'] == 'test'].drop(['Junk', 'data'], axis=1)\n",
    "\n",
    "# --- Validation split ---\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# --- LightGBM dataset ---\n",
    "lgb_train = lgb.Dataset(X_tr, y_tr)\n",
    "lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "# --- LightGBM parameters ---\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'num_leaves': 31,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'min_gain_to_split': 0.01,\n",
    "    'max_bin': 512,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# --- Train ---\n",
    "print(\"\\nTraining LightGBM...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Validate ---\n",
    "val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "val_auc = roc_auc_score(y_val, val_pred)\n",
    "print(f\"\\nValidation ROC-AUC: {val_auc:.4f}\")\n",
    "\n",
    "# --- Final model on full data ---\n",
    "full_train = lgb.Dataset(x_train, y_train)\n",
    "final_model = lgb.train(\n",
    "    params,\n",
    "    full_train,\n",
    "    num_boost_round=model.best_iteration\n",
    ")\n",
    "\n",
    "# --- Predict on test ---\n",
    "test_pred = final_model.predict(x_test, num_iteration=final_model.best_iteration)\n",
    "\n",
    "# --- Create submission: only Junk column, order matches test data ---\n",
    "submission = pd.DataFrame({\n",
    "    'Junk': test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(r'E:/IITK/P5 data/submission2.csv', index=False)\n",
    "print(\"Saved:submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a6a629b-5d98-494b-b7a7-45d6ef96607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train =  pd.read_csv(r'E:/IITK/P5 data/Property_train.csv')\n",
    "\n",
    "# Compute missing percentage\n",
    "missing_count = train['EnvRating'].isnull().sum()\n",
    "total_count = len(train)\n",
    "\n",
    "missing_percentage = (missing_count / total_count) * 100\n",
    "print(round(missing_percentage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e9fd987-e5df-4947-9a72-6dd26a21ff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train =  pd.read_csv(r'E:/IITK/P5 data/Property_train.csv')\n",
    "# Convert ListDate to datetime\n",
    "train['ListDate'] = pd.to_datetime(train['ListDate'], errors='coerce')\n",
    "\n",
    "# Extract month and compute frequency\n",
    "month_counts = train['ListDate'].dt.month.value_counts()\n",
    "\n",
    "# Get the month with the highest count\n",
    "highest_month = month_counts.idxmax()\n",
    "print(highest_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cd2707a-4c9c-4ca7-b5ee-091813d93833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT1\n"
     ]
    }
   ],
   "source": [
    "# Filter for AreaIncomeType 'A' and count Agency occurrences\n",
    "agency_counts = train[train['AreaIncomeType'] == 'A']['Agency'].value_counts()\n",
    "\n",
    "# Get the agency with the highest count\n",
    "top_agency = agency_counts.idxmax()\n",
    "print(top_agency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4556e85c-99dc-42b6-8aea-ca5b6a9ba054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR\n"
     ]
    }
   ],
   "source": [
    "# Group by State and compute median PropertyAge\n",
    "median_age_by_state = train.groupby('State')['PropertyAge'].median()\n",
    "\n",
    "# Get the State with the highest median PropertyAge\n",
    "top_state = median_age_by_state.idxmax()\n",
    "print(top_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b59197e8-60cb-4aca-b704-3603d7360be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6066959\n"
     ]
    }
   ],
   "source": [
    "# Convert PriceIndex1 to numeric, forcing errors to NaN\n",
    "train['PriceIndex1'] = pd.to_numeric(train['PriceIndex1'], errors='coerce')\n",
    "\n",
    "# Compute variance and round\n",
    "variance = round(train['PriceIndex1'].var())\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ecf09d9-76d3-4821-8c0b-ec6dc4012a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73381\n"
     ]
    }
   ],
   "source": [
    "# Convert NormalisedPopulation to numeric if needed\n",
    "train['NormalisedPopulation'] = pd.to_numeric(train['NormalisedPopulation'], errors='coerce')\n",
    "\n",
    "# Compute mean for Architecture 'YIK5'\n",
    "avg_pop = round(train.loc[train['Architecture'] == 'YIK5', 'NormalisedPopulation'].mean())\n",
    "print(avg_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88429d92-ed00-49c1-afbc-ec471df98be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41062.0\n"
     ]
    }
   ],
   "source": [
    "# Convert PriceIndex8 to numeric if necessary\n",
    "train['PriceIndex8'] = pd.to_numeric(train['PriceIndex8'], errors='coerce')\n",
    "\n",
    "# Calculate range = max - min\n",
    "price_range = train['PriceIndex8'].max() - train['PriceIndex8'].min()\n",
    "print(price_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5d7da-e4bb-479e-980d-3582541dd63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
